
import requests
from bs4 import BeautifulSoup

def investorsHubScraper(url):

    #url = "https://investorshub.advfn.com/Gamestop-Corporation-GME-4617"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',
    }

    #n = 0
    counter = -1# at the end of board you reach a ssection that is just 0 to 1 comments a page

    while url != None and counter != 0 and counter != 1:
        counter = 0
    
        response = requests.get(url, headers=headers)
        # Check if the request was successful (status code 200
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            comments = soup.find_all("div", {"class": "card full-message-card my-4 mt-md-0 mb-md-2 rounded-0 border-left-0 border-right-0 border-top-0"})
            comment_info = []
            
            for comment in comments:
                counter += 1
                time = comment.find("div", {"data-app": "message-timestamp"}).get_text().strip()
                text = comment.find("p", {"class": "mb-0"})
                user = comment.find("a" , {"class": "text-blue-link message-author font-weight-bold pt-0 mt-0"}).get_text().strip()
                replies = comment.find("span", {"class":"reaction-text"}).get_text().strip()
                if replies == "Reply":
                    reply_number = 0
                else:
                    reply_number = replies[0:replies.index(" ")]


                text = text.get_text()
                # break into lines and remove leading and trailing space on each
                lines = (line.strip() for line in text.splitlines())
                # break multi-headlines into a line each
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                comment_info.append([time,user,text,reply_number])



            """ 
            #for testing. need to write to database
            f = open("scrapedcomments"+str(n)+".txt", "w", encoding="utf-8")
            for i in comment_info:
                f.write(i[0]+"\n")
                f.write(i[1]+"\n")
                f.write(i[2]+"\n")
                f.write(str(i[3])+"\n\n")
            
            f.close()
            """


        buttons = soup.find_all("a", {"class":"mt-1 mr-2 btn blue-style-btn text-decoration-none"})
        url = None
        for b in buttons:
            if b.get_text().strip() == "Older":
                url = b.get("href")
                break

        #n = n +1
